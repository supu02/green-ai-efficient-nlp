## ğŸ§  Green AI: Energy-Efficient NLP Under Deployment Constraints

Energy-aware sentiment analysis benchmarking classical and transformer models under CPU-only deployment constraints.

â¸»

Overview

This project investigates how to balance predictive performance and environmental impact in modern NLP systems.

While transformer models achieve strong accuracy, they often require substantial computational resources. This work evaluates whether compact architectures and quantization techniques can maintain competitive performance while significantly reducing carbon footprint and inference cost.

The focus is not only accuracy â€” but deployment realism.

â¸»

## ğŸ¯ Objective

To evaluate the trade-offs between:

	â€¢	Predictive performance
	â€¢	Inference efficiency
	â€¢	Carbon footprint (GWP)
	â€¢	CPU-only feasibility

Key questions:

	â€¢	How much accuracy is gained by transformers over classical baselines?
	â€¢	What is the environmental cost of that gain?
	â€¢	Can quantization reduce impact while preserving performance?
	â€¢	What configuration offers the best accuracy-to-carbon ratio?

â¸»

## ğŸ“¦ Experimental Setup

Dataset

	â€¢	IMDb Movie Review Dataset
	â€¢	Binary sentiment classification (positive / negative)

Deployment Constraint

	â€¢	CPU-only inference
	â€¢	No GPU acceleration
	â€¢	Lightweight runtime environment

All models were evaluated under consistent runtime conditions.

â¸»

## ğŸ§ª Model Variants

Model Variant	Description
TF-IDF + Linear SVM	Classical sparse feature baseline
TinyBERT (Fine-Tuned)	Compact transformer model
DistilBERT Cascade	Two-stage confidence-based pipeline
Quantized TinyBERT (INT8)	Post-training quantized transformer


â¸»

## âš™ Methodology

	1.	Text preprocessing and tokenization
	2.	TF-IDF feature extraction for classical baseline
	3.	Fine-tuning compact transformer models
	4.	Post-training INT8 quantization
	5.	Benchmarking under CPU-only constraints
	6.	Evaluation using:
	â€¢	Accuracy
	â€¢	Carbon footprint (GWP)
	â€¢	Efficiency trade-off analysis

Carbon impact values were obtained from official competition evaluation logs.

â¸»

## ğŸ“Š Evaluation Results

Green AI Benchmark â€” Team â€œnousâ€

| Model Variant              | Accuracy    | Total COâ‚‚ (GWP)      | Key Observation                          |
|----------------------------|------------|----------------------|------------------------------------------|
| TF-IDF + Linear SVM        | 0.78â€“0.79  | ~0.00001â€“0.00002     | Extremely low footprint, strong baseline |
| Quantized Transformer      | ~0.79      | ~0.00007â€“0.00008     | Best accuracyâ€“impact trade-off           |
| Fine-Tuned Transformer     | ~0.80      | ~0.00013+            | Highest accuracy, highest impact         |

All results correspond to official competition submissions recorded under team name â€œnousâ€.

â¸»

ğŸ“ˆ Key Findings

	â€¢	Classical models remain highly competitive under strict efficiency constraints.
	â€¢	Compact transformers provide measurable accuracy gains.
	â€¢	Quantization significantly reduces environmental cost.
	â€¢	Accuracy improvements come with non-trivial carbon trade-offs.
	â€¢	Deployment-aware benchmarking changes model selection decisions.

â¸»

## ğŸ§  Engineering Perspective

This project emphasizes:

	â€¢	Deployment-aware model selection
	â€¢	Fair benchmarking under fixed runtime conditions
	â€¢	Carbon-aware ML evaluation
	â€¢	Trade-off analysis over leaderboard chasing
	â€¢	Practical system constraints

â¸»

## ğŸ— Project Structure

```
green-ai-efficient-nlp/
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_svm_supriya.py
â”‚   â”œâ”€â”€ train_distilbert_supriya.py
â”‚   â”œâ”€â”€ ensemble_svm.py
â”‚   â”œâ”€â”€ tinybert_model.py
â”‚   â”œâ”€â”€ cascade_model.py
â”‚   â”œâ”€â”€ green_ai_model.py
â”‚   â””â”€â”€ char_model.py
â”œâ”€â”€ results/
â”œâ”€â”€ figures/
â””â”€â”€ docs/
```

â¸»

## ğŸš€ Possible Extensions

	â€¢	Knowledge distillation comparison
	â€¢	Structured pruning experiments
	â€¢	ONNX export for optimized CPU runtime
	â€¢	Direct energy instrumentation
	â€¢	Extended carbon benchmarking

â¸»

## ğŸ§  What This Project Demonstrates

	â€¢	Practical model compression
	â€¢	Quantization-aware evaluation
	â€¢	Efficiency-focused transformer deployment
	â€¢	Classical vs deep learning trade-off analysis
	â€¢	Real-world ML decision thinking

â¸»

## Status

Competition prototype completed.
Code structured for reproducibility and public demonstration.


